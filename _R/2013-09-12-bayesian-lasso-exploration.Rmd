---
layout: post
title: "Bayesian LASSO"
description: ""
category: [Teaching]
tags: [STAT 615,R,logistic regression,LASSO]
---

{% include JB/setup %}

## LASSO

Taking a look at the diabetes data

```{r}
library(lars)
data(diabetes)
summary(diabetes$x)
```

The standardization that has occurred here is that each explanatory variable "has been standardized to have unit L2 norm in each column and zero mean". So

```{r}
colSums(diabetes$x^2)
```

Let's look at the least squares estimates just to get an idea of the magnitude of effects we are expecting to see. 

```{r}
m = lm(y~x, diabetes)
m
```

Now we can look at the LASSO trace. 

```{r}
plot(l <- lars(diabetes$x, diabetes$y))
```

Here the x-axis represents a re-scaled penalty parameter and the y-axis is the estimated coefficient with that penalty. The vertical lines indicate the times when a variable comes in or goes out of the model. Since there are only 10 covariates and 12 vertical lines, one variables comes in and out of the model. This can be verified by printing the lars object (below) where we can see `hdl` comes into the model at step 4, out at step 11, and back in at step 12. 

```{r}
l
```

### Bayesian LASSO

Now we take a look at the Bayesian LASSO as implemented in the package `monomvn`. 

```{r}
library(monomvn)
attach(diabetes)

## Ordinary Least Squares regression
reg.ols <- regress(x, y)

## Lasso regression with the penalty choosen by leave-one-out cross validation
reg.las <- regress(x, y, method="lasso", validation="LOO")

# Fully Bayesian LASSO
reg.blas <- blasso(x, y, RJ=FALSE)

## summarize the beta (regression coefficients) estimates
plot(reg.blas, burnin=200)
points(drop(reg.las$b), col=2, pch=20)
points(drop(reg.ols$b), col=3, pch=18)
legend("topleft", c("blasso-map", "lasso", "lsr"),
       col=c(2,2,3), pch=c(21,20,18))
```

This plot provides a comparison betweenthe fully Bayesian answer (provided via boxplots) and the LASSO (with the penalty chosen using LOO x-validation) vs OLS estimates. 
